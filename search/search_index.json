{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"blockfin-docs","text":""},{"location":"#dencun","title":"Dencun","text":""},{"location":"#linky","title":"Linky","text":"<ul> <li>Registra\u010dn\u00fd formul\u00e1r (na pr\u00edstup)</li> <li>Sheet harmonogram (rezerovanie si \u010dasu)</li> </ul>"},{"location":"#navody","title":"N\u00e1vody","text":"<ul> <li>Dencun SSH Access Guide</li> <li>Ako vola\u0165 LLM pomocou Ollama?</li> </ul>"},{"location":"#litellm","title":"LiteLLM","text":"<ul> <li>LiteLLM Quickstart</li> </ul>"},{"location":"dencun/","title":"Dencun SSH Access Guide","text":"<p>Tento tutorial sl\u00fa\u017ei na nastavenie SSH pr\u00edstupu k serveru Dencun a n\u00e1sledne ako pomocou tohto pr\u00edstupu pou\u017e\u00edva\u0165 Ollama service, ktor\u00fd vyu\u017e\u00edve na pr\u00e1cu s LLM modelmi.</p> <ul> <li>SSH pr\u00edstupu - pomocou termin\u00e1lu alebo VS Code/in\u00fd editor</li> <li>Dencun (2x4090) - je n\u00e1zov servera/stroja v 5.44, ku ktor\u00e9mu sa prip\u00e1jate pomocou SSH alebo fyzicky.</li> <li>Google registra\u010dn\u00fd formul\u00e1r na registr\u00e1ciu \u017eiadost o prihl\u00e1senie - link (prihlasenie pomocou stuba emailu)</li> <li>Google sheet harmonogram na rezerv\u00e1ciu \u010dasu na pr\u00e1cu so serverom - link (prihlasenie pomocou stuba emailu)</li> </ul>"},{"location":"dencun/#dolezite-upozornenia","title":"D\u00f4le\u017eit\u00e9 upozornenia","text":"<ul> <li>V\u00fdpo\u010dtov\u00fd v\u00fdkon servera je obmedzen\u00fd, preto sa sna\u017ete optimalizova\u0165 va\u0161e po\u017eiadavky a nevy\u0165a\u017eova\u0165 server zbyto\u010dne.</li> <li>Tento server sa zdiela medzi viacer\u00fdmi pou\u017e\u00edvate\u013emi a preto je potrebn\u00e9 sa prihlasova\u0165/rezervova\u0165 si \u010das na pr\u00e1cu s n\u00edm. Dokument cez, ktor\u00fd sa registrujete - alokujete si svoj slot je dostupn\u00fd tu (google sheet) (prihlasenie pomocou stuba emailu)</li> </ul>"},{"location":"dencun/#krok-1-vyplenenie-formulara","title":"Krok 1: Vyplenenie formul\u00e1ra","text":"<p>Pre z\u00edskanie pr\u00edstupu na server je potrebn\u00e9 vyplni\u0165 tento formul\u00e1r (prihlasenie pomocou stuba emailu). N\u00e1sledne v\u00e1m do p\u00e1r dn\u00ed bude vytvoren\u00e9 konto a bude v\u00e1m bude pridelen\u00e9 prihlasovacie meno (pre \u0161tudentov je to ais id - xmeno, pre zamestnancov meno.priezvisko).</p>"},{"location":"dencun/#krok-2-generovanie-ssh-klucov","title":"Krok 2: Generovanie SSH k\u013e\u00fa\u010dov","text":"<p>Na z\u00edskanie pr\u00edstupu k serveru potrebujete SSH k\u013e\u00fa\u010d, ktor\u00e9 sa pou\u017e\u00edva na autentifik\u00e1ciu. Tento k\u013e\u00fa\u010d sa generuje pomocou pr\u00edkazu, ktor\u00fd treba spusti\u0165 v termin\u00e1li.</p>"},{"location":"dencun/#1-ssh-kluc-pre-fiit","title":"1. SSH k\u013e\u00fa\u010d pre FIIT","text":"<pre><code>ssh-keygen -t ed25519 -C \"vas_email@stuba.sk\" -f ~/.ssh/fiit_internal\n</code></pre> <p>Pozn\u00e1mky k SSH k\u013e\u00fa\u010dom: - K\u013e\u00fa\u010d sa vytvor\u00ed v adres\u00e1ri <code>~/.ssh/</code> - Pre k\u013e\u00fa\u010d sa vytvoria dva s\u00fabory:   - Priv\u00e1tny k\u013e\u00fa\u010d (<code>fiit_internal</code>) - uchov\u00e1vajte ho v bezpe\u010d\u00ed a nikdy ho nezdie\u013eajte   - Verejn\u00fd k\u013e\u00fa\u010d (<code>fiit_internal.pub</code>)</p>"},{"location":"dencun/#krok-3-konfiguracia-ssh","title":"Krok 3: Konfigur\u00e1cia SSH","text":"<p>Pre jednoduch\u00e9 pripojenie vytvorte alebo upravte konfigura\u010dn\u00fd s\u00fabor SSH:</p> <ol> <li>Otvorte alebo vytvorte s\u00fabor <code>~/.ssh/config</code>:</li> </ol> <pre><code>vim ~/.ssh/config\n</code></pre> <ol> <li>Pridajte nasleduj\u00facu konfigur\u00e1ciu (nahra\u010fte <code>[vase_prihlasovacie_meno]</code> va\u0161\u00edm skuto\u010dn\u00fdm ais id menom - xmeno (studenti), meno.priezvisko (zamestnanci)):</li> </ol> <pre><code>Host blockaccess\n    Hostname 147.175.150.111\n    Port 8022\n    User extaccess\n    IdentityFile ~/.ssh/fiit_internal\n\nHost dencun\n    Hostname dencun\n    Port 8822\n    User [vase_prihlasovacie_meno]\n    IdentityFile ~/.ssh/fiit_internal\n    ProxyJump blockaccess\n</code></pre>"},{"location":"dencun/#krok-3-pripojenie-na-server","title":"Krok 3: Pripojenie na server","text":""},{"location":"dencun/#pomocou-terminalu","title":"Pomocou termin\u00e1lu","text":"<p>Po spr\u00e1vnej konfigur\u00e1cii sa m\u00f4\u017eete pripoji\u0165 jednoducho pr\u00edkazom:</p> <pre><code>ssh dencun\n</code></pre>"},{"location":"dencun/#pomocou-vs-code","title":"Pomocou VS Code","text":"<ol> <li>Nain\u0161talujte roz\u0161\u00edrenie Remote - SSH v VS Code</li> <li>Stla\u010dte <code>Ctrl+Shift+P</code> (alebo <code>Cmd+Shift+P</code> na MacOS)</li> <li>Nap\u00ed\u0161te \"Remote-SSH: Connect to Host...\" a zvolte mo\u017enos\u0165 <code>Remote-SSH: Connect to Host...</code></li> <li>Vyberte <code>dencun</code> zo zoznamu</li> </ol>"},{"location":"litellm-quickstart/","title":"TLDR","text":"In\u00a0[2]: Copied! <pre>!uv add litellm python-dotenv \n</pre> !uv add litellm python-dotenv  <pre>Resolved 116 packages in 19ms\nAudited 113 packages in 26ms\n</pre> In\u00a0[3]: Copied! <pre>import litellm\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n</pre> import litellm import os from dotenv import load_dotenv  load_dotenv() Out[3]: <pre>True</pre> <p>Treba ma\u0165 vytvoren\u00fd s\u00fabor .env s premenn\u00fdmi API_KEY a API_BASE:</p> <pre>API_KEY=your_api_key_here\nAPI_BASE=https://your_api_base_here\n</pre> <p>API_KEY a API_BASE v\u00e1m boli dodan\u00e9 ved\u00facim.</p> In\u00a0[4]: Copied! <pre>API_KEY = os.getenv(\"API_KEY\")\nAPI_BASE = os.getenv(\"API_BASE\")\n</pre> API_KEY = os.getenv(\"API_KEY\") API_BASE = os.getenv(\"API_BASE\") In\u00a0[7]: Copied! <pre>response = litellm.completion(\n    model=\"openai/gpt-4.1-nano-fiit\", # model ID\n    messages=[{\"content\": \"Hello, how are you?\", \"role\": \"user\"}], # chat messages v openai formate (user, system, assistant)\n    api_key=API_KEY,\n    api_base=API_BASE,\n)\nresponse\n</pre> response = litellm.completion(     model=\"openai/gpt-4.1-nano-fiit\", # model ID     messages=[{\"content\": \"Hello, how are you?\", \"role\": \"user\"}], # chat messages v openai formate (user, system, assistant)     api_key=API_KEY,     api_base=API_BASE, ) response Out[7]: <pre>ModelResponse(id='chatcmpl-CVk7l5fopnhxn89Y58B8cicVwioOy', created=1761681505, model='gpt-4.1-nano-2025-04-14', object='chat.completion', system_fingerprint='fp_950f36939b', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! I'm doing well, thank you. How can I assist you today?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=16, prompt_tokens=13, total_tokens=29, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')</pre> In\u00a0[10]: Copied! <pre>!curl -X GET \"{API_BASE}/v1/models\" -H \"Authorization: Bearer {API_KEY}\"\n</pre> !curl -X GET \"{API_BASE}/v1/models\" -H \"Authorization: Bearer {API_KEY}\" <pre>{\"data\":[{\"id\":\"gpt-4.1-nano-fiit\",\"object\":\"model\",\"created\":1677610602,\"owned_by\":\"openai\"},{\"id\":\"gpt-4.1-mini-fiit\",\"object\":\"model\",\"created\":1677610602,\"owned_by\":\"openai\"},{\"id\":\"gpt-5-fiit\",\"object\":\"model\",\"created\":1677610602,\"owned_by\":\"openai\"},{\"id\":\"claude-haiku-4.5-fiit\",\"object\":\"model\",\"created\":1677610602,\"owned_by\":\"openai\"},{\"id\":\"gpt-4.1-fiit\",\"object\":\"model\",\"created\":1677610602,\"owned_by\":\"openai\"},{\"id\":\"claude-sonnet-4.5-fiit\",\"object\":\"model\",\"created\":1677610602,\"owned_by\":\"openai\"}],\"object\":\"list\"}</pre> <p>V\u0161etky modely, ktor\u00e9 s\u00fa dostupn\u00e9 pre tvoj API_KEY a API_BASE</p> <ul> <li>ako \"model\" parameter do litellm zad\u00e1va\u0161 \"id\", tak\u017ee <code>openai/gpt-4.1-nano-fiit</code> a nie <code>openai/gpt-4.1</code></li> </ul>"},{"location":"litellm-quickstart/#tldr","title":"TLDR\u00b6","text":""},{"location":"litellm-quickstart/#viac-info","title":"Viac info\u00b6","text":"<ul> <li>litellm dokument\u00e1cia: litellm.ai/docs</li> <li>batch call - batching_completion</li> <li>steaming - stream+async</li> </ul>"},{"location":"ollama/","title":"Ollama","text":""},{"location":"ollama/#ako-pouzivat-ollama-spolocne-s-dencun-om","title":"Ako pou\u017e\u00edva\u0165 Ollama spolo\u010dne s Dencun-om","text":"<p>Tento n\u00e1vod v\u00e1m uk\u00e1\u017ee, ako sa pripoji\u0165 k slu\u017ebe Ollama, ktor\u00e1 be\u017e\u00ed na serveri Dencun, a ako ju pou\u017e\u00edva\u0165 vo va\u0161ich aplik\u00e1ci\u00e1ch.</p>"},{"location":"ollama/#co-je-ollama","title":"\u010co je Ollama?","text":"<p>Ollama je n\u00e1stroj, ktor\u00fd umo\u017e\u0148uje jednoducho sp\u00fa\u0161\u0165a\u0165 a spravova\u0165 ve\u013ek\u00e9 jazykov\u00e9 modely (LLM) lok\u00e1lne. V na\u0161om pr\u00edpade be\u017e\u00ed Ollama na serveri Dencun a poskytuje API rozhranie, cez ktor\u00e9 m\u00f4\u017eeme t\u00fdmto modelom posiela\u0165 po\u017eiadavky.</p> <ul> <li>Ollama docs: https://ollama.com/docs</li> </ul>"},{"location":"ollama/#predpoklady","title":"\u203c\ufe0f Predpoklady \u203c\ufe0f","text":"<p>Ne\u017e za\u010dnete, mus\u00edte ma\u0165: * Nastaven\u00fd a funk\u010dn\u00fd SSH pr\u00edstup na server <code>dencun</code>.</p>"},{"location":"ollama/#1-pripojenie-k-ollama-api","title":"1. Pripojenie k Ollama API","text":"<p>Existuj\u00fa dva hlavn\u00e9 sp\u00f4soby, ako sa k Ollama API pripoji\u0165. </p>"},{"location":"ollama/#sposob-a-chcem-pracovat-z-mojho-lokalneho-pocitaca","title":"Sp\u00f4sob A: Chcem pracova\u0165 z m\u00f4jho lok\u00e1lneho po\u010d\u00edta\u010da","text":"<p>Ak chcete API vola\u0165 z k\u00f3du, ktor\u00fd be\u017e\u00ed na va\u0161om osobnom po\u010d\u00edta\u010di (napr. vo VS Code na va\u0161om laptope), mus\u00edte si \"presmerova\u0165\" port zo servera Dencun k sebe pomocou port forwardingu. </p> <p>Do termin\u00e1lu zadajte nasleduj\u00faci pr\u00edkaz:</p> <pre><code>ssh -L 11434:localhost:11434 dencun\n</code></pre> <ul> <li>Vysvetlenie: Tento pr\u00edkaz hovor\u00ed: \"V\u0161etko, \u010do po\u0161lem na <code>localhost:11434</code> na mojom po\u010d\u00edta\u010di, presmeruj cez SSH na <code>localhost:11434</code> na serveri <code>dencun</code>.\"</li> <li>D\u00f4le\u017eit\u00e9: Nechajte tento termin\u00e1l otvoren\u00fd po cel\u00fd \u010das, k\u00fdm chcete pripojenie pou\u017e\u00edva\u0165.</li> </ul> <p>Teraz je pre v\u00e1s Ollama API dostupn\u00e9 na adrese <code>http://localhost:11434</code>.</p>"},{"location":"ollama/#sposob-b-chcem-pracovat-priamo-na-serveri-dencun","title":"Sp\u00f4sob B: Chcem pracova\u0165 priamo na serveri Dencun","text":"<p>Ak sp\u00fa\u0161\u0165ate svoj k\u00f3d (napr. Python skript) priamo na serveri Dencun, nepotrebujete \u017eiadny port forwarding.</p> <ol> <li>Jednoducho sa prihl\u00e1ste na server:     <code>bash     ssh dencun</code></li> <li>Ke\u010f\u017ee v\u00e1\u0161 k\u00f3d be\u017e\u00ed na tom istom stroji ako Ollama, API je pre v\u00e1s automaticky dostupn\u00e9 na adrese <code>http://localhost:11434</code>.</li> </ol>"},{"location":"ollama/#2-overenie-funkcnosti","title":"2. Overenie funk\u010dnosti","text":"<p>Bez oh\u013eadu na to, ktor\u00fd scen\u00e1r ste si vybrali, m\u00f4\u017eete funk\u010dnos\u0165 overi\u0165 rovnako.</p> <p>Otvorte vo va\u0161om webovom prehliada\u010di (ak pou\u017e\u00edvate Scen\u00e1r A) alebo pou\u017eite n\u00e1stroj <code>curl</code> v termin\u00e1li (funguje pre oba scen\u00e1re) nasleduj\u00facu adresu:</p> <p><code>http://localhost:11434/api/tags</code></p> <p>O\u010dak\u00e1van\u00fd v\u00fdsledok: Mali by ste vidie\u0165 odpove\u010f vo form\u00e1te JSON, ktor\u00e1 obsahuje zoznam v\u0161etk\u00fdch LLM modelov dostupn\u00fdch na serveri Dencun.</p> <pre><code>{\n  \"models\": [\n    {\n      \"name\": \"llama3:latest\",\n      \"modified_at\": \"...\",\n      \"size\": ...\n    },\n    {\n      \"name\": \"mistral:latest\",\n      \"modified_at\": \"...\",\n      \"size\": ...\n    }\n    // ... a \u010fal\u0161ie modely\n  ]\n}\n</code></pre>"},{"location":"ollama/#3-priklad-pouzitia-v-pythone-pomocou-litellm","title":"3. Pr\u00edklad pou\u017eitia v Pythone (pomocou litellm)","text":"<p>Kni\u017enica litellm zjednodu\u0161uje volanie r\u00f4znych API. Ke\u010f\u017ee ste sa (v oboch scen\u00e1roch) pripojili k localhost:11434, k\u00f3d bude pre oba pr\u00edpady rovnak\u00fd.</p> <p>Uistite sa, \u017ee m\u00e1te kni\u017enicu nain\u0161talovan\u00fa:</p> <pre><code>pip install litellm\n</code></pre> <p>Potom ju m\u00f4\u017eete pou\u017ei\u0165 v Pythone takto:</p> <pre><code>from litellm import completion\nimport os\n\n\napi_base_url = \"http://localhost:11434\"\nmodel_name = \"ollama/llama3:latest\"  # format : \"ollama/MENO_MODELU\"\n\nmessages = [\n    {\"content\": \"respond in 20 words. who are you?\", \"role\": \"user\"}\n]\n\nresponse = completion(\n        model=model_name, \n        messages=messages, \n        api_base=api_base_url\n    )\nprint(response.choices[0].message.content)\n</code></pre>"}]}